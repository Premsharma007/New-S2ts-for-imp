Batch Processing System
Excellent idea! Batch processing is essential for production workflows. Here's my implementation approach:

1. UI Toggle & Folder Selection:
python
# In main.py - UI section
with gr.Group(elem_classes="shadow-card"):
    gr.Markdown("### ðŸ“¦ Processing Mode")
    
    processing_mode = gr.Radio(
        choices=["Single File", "Batch Folder"],
        value="Single File",
        label="Processing Mode"
    )
    
    # Dynamic UI - shows either file upload or folder selection
    audio_input = gr.Audio(label="Upload Input Audio", type="filepath", visible=True)
    batch_folder = gr.File(
        label="Select Folder with Audio Files", 
        file_count="directory", 
        visible=False
    )
    
    # Batch options
    batch_options = gr.Accordion("Batch Settings", open=False, visible=False)
    with batch_options:
        batch_max_files = gr.Slider(
            minimum=1, maximum=100, value=10, 
            label="Maximum Files to Process"
        )
        batch_shuffle = gr.Checkbox(
            label="Shuffle Processing Order", value=False
        )
2. Dynamic UI Switching:
python
# Toggle between single file and batch mode
def toggle_processing_mode(mode):
    if mode == "Single File":
        return [
            gr.update(visible=True),   # audio_input
            gr.update(visible=False),  # batch_folder
            gr.update(visible=False),  # batch_options
            gr.update(visible=True)    # reference section
        ]
    else:
        return [
            gr.update(visible=False),  # audio_input
            gr.update(visible=True),   # batch_folder
            gr.update(visible=True),   # batch_options
            gr.update(visible=False)   # reference section (use defaults)
        ]

processing_mode.change(
    fn=toggle_processing_mode,
    inputs=[processing_mode],
    outputs=[audio_input, batch_folder, batch_options, ref_audio_group]
)
3. Batch Processor Class:
python
# In utils/batch_processor.py
import threading
from queue import Queue
from pathlib import Path

class BatchProcessor:
    def __init__(self, pipeline_fn, max_workers=2):
        self.pipeline_fn = pipeline_fn
        self.max_workers = max_workers
        self.job_queue = Queue()
        self.results = {}
        self.current_job = None
        self.is_processing = False
        
    def add_jobs(self, audio_files, settings):
        """Add multiple audio files to processing queue"""
        for audio_path in audio_files:
            self.job_queue.put({
                'audio_path': audio_path,
                'settings': settings,
                'status': 'queued'
            })
    
    def process_batch(self, progress_callback=None):
        """Process all jobs in the queue"""
        self.is_processing = True
        processed_count = 0
        total_count = self.job_queue.qsize()
        
        while not self.job_queue.empty() and self.is_processing:
            job = self.job_queue.get()
            self.current_job = job
            
            try:
                # Update job status
                job['status'] = 'processing'
                
                if progress_callback:
                    progress_callback(
                        processed_count, total_count,
                        f"Processing {Path(job['audio_path']).name} "
                        f"({processed_count+1}/{total_count})"
                    )
                
                # Run the pipeline for this file
                result = self.pipeline_fn(
                    audio_path=job['audio_path'],
                    **job['settings']
                )
                
                job['status'] = 'completed'
                job['result'] = result
                self.results[job['audio_path']] = job
                
                processed_count += 1
                
            except Exception as e:
                job['status'] = 'failed'
                job['error'] = str(e)
                self.results[job['audio_path']] = job
        
        self.is_processing = False
        return self.results
    
    def stop_processing(self):
        """Stop batch processing"""
        self.is_processing = False
        if self.current_job:
            self.current_job['status'] = 'cancelled'
4. Enhanced Pipeline Function:
python
# In main.py - Modified pipeline function
def run_pipeline(audio_path, settings, progress_callback=None, job_id=None):
    """
    Enhanced pipeline function that supports both single and batch processing
    """
    # Add job context to progress messages
    def job_aware_progress(stage, percent, message):
        if job_id:
            message = f"[Job {job_id}] {message}"
        if progress_callback:
            progress_callback(stage, percent, message)
    
    # Run the pipeline with job context
    return _run_pipeline_internal(audio_path, settings, job_aware_progress)
5. Batch Progress Tracking:
python
# Specialized progress for batch processing
def update_batch_progress(processed, total, current_file):
    """Update batch-level progress"""
    overall_percent = (processed / total) * 100 if total > 0 else 0
    
    return {
        batch_overall_progress: overall_percent,
        batch_current_file: current_file,
        batch_processed_count: processed,
        batch_total_count: total
    }
6. Batch UI Elements:
python
# Additional UI elements for batch mode
with gr.Group(visible=False) as batch_progress_group:
    gr.Markdown("### ðŸ“Š Batch Progress")
    
    batch_overall_progress = gr.Slider(
        minimum=0, maximum=100, value=0,
        label="Overall Progress", interactive=False
    )
    
    batch_current_file = gr.Textbox(
        label="Currently Processing", interactive=False
    )
    
    with gr.Row():
        batch_processed_count = gr.Number(label="Processed", interactive=False)
        batch_total_count = gr.Number(label="Total Files", interactive=False)
    
    batch_results = gr.Dataframe(
        label="Batch Results",
        headers=["File", "Status", "ASR Time", "TTS Time", "Error"],
        interactive=False
    )
7. Batch Execution Logic:
python
# In main.py - Batch processing handler
def run_batch_processing(batch_folder, settings, max_files, shuffle):
    """Handle batch processing of multiple audio files"""
    # Get all audio files from folder
    folder_path = Path(batch_folder)
    audio_files = list(folder_path.glob("*.mp3")) + list(folder_path.glob("*.wav")) + \
                  list(folder_path.glob("*.m4a")) + list(folder_path.glob("*.flac"))
    
    # Apply limits and shuffling
    if shuffle:
        import random
        random.shuffle(audio_files)
    audio_files = audio_files[:max_files]
    
    # Initialize batch processor
    from utils.batch_processor import BatchProcessor
    processor = BatchProcessor(run_pipeline)
    processor.add_jobs(audio_files, settings)
    
    # Process batch
    results = processor.process_batch(progress_callback=update_batch_progress)
    
    # Format results for display
    result_data = []
    for audio_path, job in results.items():
        result_data.append([
            Path(audio_path).name,
            job['status'],
            job.get('result', {}).get('asr_time', 0),
            job.get('result', {}).get('tts_time', 0),
            job.get('error', '')
        ])
    
    return {
        batch_results: result_data,
        batch_overall_progress: 100,
        batch_current_file: "Batch completed"
    }
8. Smart Resource Management:
python
# Resource-aware batch processing
def adaptive_batch_processing(audio_files, settings):
    """Adjust batch processing based on available resources"""
    import psutil
    
    # Check system resources
    cpu_count = psutil.cpu_count()
    available_ram = psutil.virtual_memory().available / (1024 ** 3)  # GB
    gpu_available = torch.cuda.is_available()
    
    # Determine optimal parallel processing
    if gpu_available and available_ram > 8:  # 8GB RAM available
        max_workers = min(2, cpu_count // 2)  # Conservative for GPU memory
    else:
        max_workers = 1  # Sequential processing
    
    return process_with_workers(audio_files, settings, max_workers)
9. Batch Output Organization:
python
# Organized output structure for batch processing
def create_batch_output_structure(input_folder):
    """Create organized output folder structure"""
    input_path = Path(input_folder)
    batch_output_dir = PROJECTS_DIR / f"Batch-{input_path.name}-{time.strftime('%Y%m%d-%H%M%S')}"
    
    # Create main output directory
    ensure_dir(batch_output_dir)
    
    # Create summary file
    summary_file = batch_output_dir / "batch_summary.json"
    
    return batch_output_dir, summary_file
10. Complete Integration:
python
# Final execution logic
def execute_processing(processing_mode, audio_input, batch_folder, settings):
    """Main execution function that handles both single and batch processing"""
    if processing_mode == "Single File":
        # Single file processing
        return run_pipeline(audio_input, settings)
    else:
        # Batch processing
        return run_batch_processing(batch_folder, settings)
Usage Flow:
User selects "Batch Folder" mode

UI shows folder selection instead of file upload

User selects folder containing audio files

Optional: Adjust batch settings (max files, shuffle)

Click Run: Processes all files sequentially

See progress: Overall progress + current file

Get results: Summary table with status for each file